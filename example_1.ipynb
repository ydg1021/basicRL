{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNgCes4jdKLXz1jeyjc+7hB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ydg1021/basicRL/blob/main/example_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "# numpy는 수학적 연산을 위한 라이브러리입니다. 여기서는 배열과 행렬 연산에 사용됩니다.\n"
      ],
      "metadata": {
        "id": "tlhPshf5VP3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def policy_evaluation_sutton_converge(policy, transition_probabilities, rewards, gamma, theta=0.0001, print_iterations=None):\n",
        "    # policy: 각 상태에서 취할 수 있는 행동의 확률\n",
        "    # transition_probabilities: 상태와 행동에 따른 다음 상태로의 전이 확률\n",
        "    # rewards: 각 행동에 대한 보상\n",
        "    # gamma: 감가율(미래 보상의 현재 가치를 결정)\n",
        "    # theta: 수렴 판단을 위한 임계값\n",
        "    # print_iterations: 출력할 반복 횟수들의 목록\n",
        "\n",
        "    num_states = policy.shape[0]  # 상태의 수\n",
        "    num_actions = policy.shape[1]  # 행동의 수\n",
        "    V = np.zeros(num_states)  # 상태 가치 초기화\n",
        "\n",
        "    iteration = 0\n",
        "    while True:\n",
        "        iteration += 1  # 반복 카운터 증가\n",
        "        delta = 0  # 가치 함수의 변화량\n",
        "        V_updated = np.zeros(num_states)  # 업데이트된 가치 함수\n",
        "\n",
        "        for s in range(num_states):\n",
        "            if (s % grid_size[1], s // grid_size[1]) in terminal_states:\n",
        "                continue  # 터미널 상태는 건너뜁니다.\n",
        "\n",
        "            v = V[s]  # 현재 상태의 가치\n",
        "            # 상태 가치 업데이트\n",
        "            V_updated[s] = sum([policy[s, a] * sum([transition_probabilities[s, a, s_prime] *\n",
        "                               (rewards[s, a, s_prime] + gamma * V[s_prime]) for s_prime in range(num_states)])\n",
        "                               for a in range(num_actions)])\n",
        "            delta = max(delta, abs(v - V_updated[s]))  # 최대 변화량 업데이트\n",
        "\n",
        "        V = V_updated.copy()  # 상태 가치 업데이트\n",
        "        # 터미널 상태의 가치를 0으로 설정\n",
        "        for terminal_state in terminal_states:\n",
        "            state_index = terminal_state[1] * grid_size[1] + terminal_state[0]\n",
        "            V[state_index] = 0\n",
        "\n",
        "        # 첫 번째 반복에서 비터미널 상태의 가치를 -1로 설정\n",
        "        if iteration == 1:\n",
        "            for x in range(grid_size[0]):\n",
        "                for y in range(grid_size[1]):\n",
        "                    if (x, y) not in terminal_states:\n",
        "                        state = x * grid_size[1] + y\n",
        "                        V[state] = -1\n",
        "\n",
        "        # 지정된 반복 횟수에서 상태 가치 출력\n",
        "        if print_iterations and iteration in print_iterations:\n",
        "            print(f\"Iteration {iteration}:\")\n",
        "            print(V.reshape(grid_size))\n",
        "            print()  # 줄바꿈\n",
        "\n",
        "        if delta < theta:  # 수렴 조건 검사\n",
        "            break\n",
        "\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "mQNkpLSEVPIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grid_size = (4, 4)  # 그리드 크기 설정 (4x4)\n",
        "num_states = grid_size[0] * grid_size[1]  # 상태의 총 수 계산\n",
        "terminal_states = [(0, 0), (3, 3)]  # 터미널 상태 설정\n",
        "gamma = 1  # 감가율 설정\n",
        "\n",
        "policy = np.full((num_states, 4), 0.25)  # 균등 무작위 정책 설정\n",
        "\n",
        "# 전이 확률 및 보상 설정\n",
        "transition_probabilities = np.zeros((num_states, 4, num_states))\n",
        "rewards = np.zeros((num_states, 4, num_states))\n",
        "# 각 상태와 행동에 대한 전이 확률 및 보상을 설정\n",
        "for s in range(num_states):\n",
        "    if (s % grid_size[1], s // grid_size[1]) in terminal_states:\n",
        "        continue\n",
        "    for a in range(4):\n",
        "        next_s = s\n",
        "        # 상, 하, 좌, 우 이동에 따른 다음 상태 결정\n",
        "        if a == 0 and s // grid_size[1] > 0:  # Up\n",
        "            next_s -= grid_size[1]\n",
        "        elif a == 1 and s // grid_size[1] < grid_size[0] - 1:  # Down\n",
        "            next_s += grid_size[1]\n",
        "        elif a == 2 and s % grid_size[1] > 0:  # Left\n",
        "            next_s -= 1\n",
        "        elif a == 3 and s % grid_size[1] < grid_size[1] - 1:  # Right\n",
        "            next_s += 1\n",
        "        transition_probabilities[s, a, next_s] = 1\n",
        "        rewards[s, a, next_s] = -1\n"
      ],
      "metadata": {
        "id": "SBm5595iVRvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print_iterations = [1, 2, 3, 10, 20, 50]  # 출력할 반복 횟수 지정\n",
        "estimated_values_converge = policy_evaluation_sutton_converge(policy, transition_probabilities, rewards, gamma, print_iterations=print_iterations)\n",
        "print(\"Estimated values after convergence:\")\n",
        "print(estimated_values_converge.reshape(grid_size))  # 최종 상태 가치 출력\n"
      ],
      "metadata": {
        "id": "WE805efwVTBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}